work_group: ${PRIMUS_TEAM:amd}
user_name: ${PRIMUS_USER:root}
exp_name: ${PRIMUS_EXP_NAME:qwen3_0.6B-dispersion-pretrain}
workspace: ./output

modules:
  pre_trainer:
    framework: torchtitan
    config: pre_trainer.yaml

    # model to run
    model: qwen3_0.6b.yaml
    overrides:

      # Dispersion loss configuration (matching midtrain_gpt2.py args)
      dispersion:
        variant: "l2_repel"        # Set to null to disable. Options: decorrelation, l2_repel, angular_spread, orthogonalization, perplexity_entropy
        dispersion_coeff: 1.0      # Weight for dispersion loss
        dispersion_loc: "all"      # "last" (final layer only) or "all" (average across layers)
        tau_l2: 0.5                # Temperature for l2_repel
        tau_cos: 0.5               # Temperature for angular_spread

      optimizer:
        name: "AdamW"
        lr: 3.0e-4
        eps: 1.0e-8

      lr_scheduler:
        warmup_steps: 2

      metrics:
        log_freq: 1
        enable_wandb: false        # Set to true to enable WandB logging

      training:
        local_batch_size: 4
        seq_len: 4096
        max_norm: 1.0
        steps: 50

      parallelism:
        data_parallel_replicate_degree: 1
        data_parallel_shard_degree: -1
        fsdp_reshard_after_forward: "default"
        tensor_parallel_degree: 1
        context_parallel_degree: 1

      checkpoint:
        enable: false
        folder: "checkpoint"
        interval: 500

      primus_turbo:
        enable_primus_turbo: true
        enable_attention_float8: false

